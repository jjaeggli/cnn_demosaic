{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae64d1-548f-4546-8d4b-52e84bac728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pyexr\n",
    "import rawpy\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cnn_demosaic import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084a846-d065-4d05-97c8-0935cc49030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Prevent TensorFlow from allocating all GPU memory.\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe29fdd-8a3b-44ca-9f58-5476e4032a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the camera processed color card\n",
    "SRGB_PATH = \"../training/DSCF5652_card_srgb.exr\"\n",
    "# The path to the demosaiced but not color corrected color card\n",
    "EXR_PATH = \"../training/DSCF5652_card.exr\"\n",
    "# The path to the raw image\n",
    "RAW_PATH = \"../training/DSCF5652.RAF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9dfbca-46b2-4f22-b02e-7659157975d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "srgb_img_arr = pyexr.read(SRGB_PATH)[:, :, :3]\n",
    "exr_img_arr = pyexr.read(EXR_PATH)[:, :, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fec6e2-3d9a-482a-9744-9ebb3cda896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pixel format and values should be floating point triad in range 0.0...1.0\n",
    "srgb_img_arr[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b30ce-0cb5-4893-bbbc-7e49d537a1bd",
   "metadata": {},
   "source": [
    "\n",
    "## Set up functional helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fabd47-d618-46ef-b473-98b42e7b1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    plt.imshow(image, vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def to_color_arr(img_arr):\n",
    "    \"\"\"Flatten the 2D image to an array.\"\"\"\n",
    "    return img_arr.reshape((img_arr.shape[0] * img_arr.shape[1], 3))\n",
    "\n",
    "\n",
    "def random_sampling(target_arr, n_samples=1000):\n",
    "    \"\"\"Returns an array containing n_samples from a color swatch.\"\"\"\n",
    "    rand_index = np.random.randint(0, target_arr.shape[0], n_samples)\n",
    "    return target_arr[rand_index]\n",
    "\n",
    "\n",
    "def plot_color_3d(col_arr):\n",
    "    r, g, b = col_arr[:, 0].flatten(), col_arr[:, 1].flatten(), col_arr[:, 2].flatten()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    axis = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "\n",
    "    axis.scatter(r, g, b, c=col_arr, marker=\"o\")\n",
    "    axis.set_xlabel(\"Red\")\n",
    "    axis.set_ylabel(\"Green\")\n",
    "    axis.set_zlabel(\"Blue\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_color_scope(img_arr):\n",
    "    x_points = []\n",
    "    c_points = []\n",
    "\n",
    "    for i in range(0, img_arr.shape[0]):\n",
    "        for j in range(img_arr.shape[1]):\n",
    "            x_points.append(j)\n",
    "            c_points.append(img_arr[i, j])\n",
    "\n",
    "    c_points = np.asarray(c_points)\n",
    "    x_points = np.asarray(x_points)\n",
    "\n",
    "    norm_c = c_points.sum(axis=1) / 3\n",
    "\n",
    "    r_vals = [(1, 0, 0, c * 0.01) for c in c_points[:, 0]]\n",
    "    g_vals = [(0, 1, 0, c * 0.01) for c in c_points[:, 1]]\n",
    "    b_vals = [(0, 0, 1, c * 0.01) for c in c_points[:, 2]]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    axis = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # Values are normalized on the red axis.\n",
    "    axis.scatter(x_points, c_points[:, 0] / c_points[:, 0], c=r_vals, s=3)\n",
    "    axis.scatter(x_points, c_points[:, 1] / c_points[:, 0], c=g_vals, s=3)\n",
    "    axis.scatter(x_points, c_points[:, 2] / c_points[:, 0], c=b_vals, s=3)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf21e8-e0bb-4ad5-9c57-548436f8d89e",
   "metadata": {},
   "source": [
    "\n",
    "## Find a color conversion matrix using least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d0279-8efa-43f4-976a-5761692f34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This finds the diagonal matrix which best matches the color transformation\n",
    "# between two swatches. Note: This does not find a transformation which\n",
    "# matches multiple color chips.\n",
    "\n",
    "\n",
    "def find_diagonal_transformation(raw_img, target_img):\n",
    "    raw_reshaped = raw_img.reshape(-1, 3)\n",
    "    target_reshaped = target_img.reshape(-1, 3)\n",
    "\n",
    "    # Solve for diagonal matrix only\n",
    "    diag_matrix = np.zeros(3)\n",
    "    for i in range(3):\n",
    "        # Use only the corresponding channel for regression\n",
    "        diag_matrix[i] = np.linalg.lstsq(\n",
    "            raw_reshaped[:, i : i + 1], target_reshaped[:, i], rcond=None\n",
    "        )[0][0]\n",
    "    # Create a diagonal transformation matrix\n",
    "    transform = np.diag(diag_matrix)\n",
    "    return transform\n",
    "\n",
    "\n",
    "def apply_color_transformation(img, transformation_matrix):\n",
    "    # Reshape image to (n_pixels, 3)\n",
    "    img_reshaped = img.reshape(-1, 3)\n",
    "    # Apply transformation\n",
    "    transformed = np.dot(img_reshaped, transformation_matrix)\n",
    "    # Reshape back to original image shape\n",
    "    transformed_img = transformed.reshape(img.shape)\n",
    "    # Clip values to valid range [0, 255] or [0, 1] depending on your image format\n",
    "    transformed_img = np.clip(transformed_img, 0, 255 if img.dtype == np.uint8 else 1.0)\n",
    "    return transformed_img.astype(img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b816f9-8195-442c-bddc-a26a5784877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sRGB and camera raw color chips from the image.\n",
    "\n",
    "srgb_chip_0 = srgb_img_arr[:160, :160]\n",
    "srgb_chip_1 = srgb_img_arr[:160, 160:320]\n",
    "srgb_chip_2 = srgb_img_arr[:160, 320:480]\n",
    "\n",
    "raw_chip_0 = exr_img_arr[:160, :160]\n",
    "raw_chip_1 = exr_img_arr[:160, 160:320]\n",
    "raw_chip_2 = exr_img_arr[:160, 320:480]\n",
    "\n",
    "# An isolated color swatch should have consistent levels across the x axis.\n",
    "plot_color_scope(raw_chip_1)\n",
    "plot_color_scope(srgb_chip_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e76514-2db5-429b-9623-566342ab1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the tile so that there are a uniform number of pixels in each array.\n",
    "raw_samples = random_sampling(to_color_arr(raw_chip_0))\n",
    "target_samples = random_sampling(to_color_arr(srgb_chip_0))\n",
    "\n",
    "# Find the matrix which transforms the raw pixels to target pixels.\n",
    "transform_arr = find_diagonal_transformation(raw_samples, target_samples)\n",
    "\n",
    "# Apply the transformation and display the transformed raw tile next to the\n",
    "# original sRGB camera-processed tile.\n",
    "t_chip_0 = apply_color_transformation(raw_chip_0, transform_arr)\n",
    "plot_color_scope(t_chip_0)\n",
    "display_image(t_chip_0)\n",
    "display_image(srgb_chip_0)\n",
    "\n",
    "# Display the transformation matrix, which shows that it is only performing\n",
    "# single channel mixing.\n",
    "display(transform_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c6f3d-af21-499c-ba8c-a9f79d17367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the same operations above for chip_1\n",
    "raw_samples = random_sampling(to_color_arr(raw_chip_1))\n",
    "target_samples = random_sampling(to_color_arr(srgb_chip_1))\n",
    "\n",
    "transform_arr = find_diagonal_transformation(raw_samples, target_samples)\n",
    "\n",
    "t_chip_1 = apply_color_transformation(raw_chip_1, transform_arr)\n",
    "plot_color_scope(t_chip_1)\n",
    "display_image(t_chip_1)\n",
    "display_image(srgb_chip_1)\n",
    "\n",
    "# The resulting transform for this tile is very different from the\n",
    "# transformation for another tile of the same image.\n",
    "display(transform_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df364f7-5c00-48f2-adf6-d08da8eaa6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the same operations above for chip_2\n",
    "raw_samples = random_sampling(to_color_arr(raw_chip_2))\n",
    "target_samples = random_sampling(to_color_arr(srgb_chip_2))\n",
    "\n",
    "transform_arr = find_diagonal_transformation(raw_samples, target_samples)\n",
    "\n",
    "display(transform_arr)\n",
    "t_chip_2 = apply_color_transformation(raw_chip_2, transform_arr)\n",
    "plot_color_scope(t_chip_2)\n",
    "display_image(t_chip_2)\n",
    "\n",
    "# The resulting transform for this tile is very different from the\n",
    "# transformation for another tile of the same image.\n",
    "display_image(srgb_chip_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af3d856-760c-4fca-849f-f2327ea4fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample all three images. Find a diagonal transformation that best fits all three samples.\n",
    "raw_samples = np.concatenate(\n",
    "    (\n",
    "        random_sampling(to_color_arr(raw_chip_0)),\n",
    "        random_sampling(to_color_arr(raw_chip_1)),\n",
    "        random_sampling(to_color_arr(raw_chip_2)),\n",
    "    )\n",
    ")\n",
    "target_samples = np.concatenate(\n",
    "    (\n",
    "        random_sampling(to_color_arr(srgb_chip_0)),\n",
    "        random_sampling(to_color_arr(srgb_chip_1)),\n",
    "        random_sampling(to_color_arr(srgb_chip_2)),\n",
    "    )\n",
    ")\n",
    "\n",
    "transform_arr = find_diagonal_transformation(raw_samples, target_samples)\n",
    "t_chip_2 = apply_color_transformation(raw_chip_2, transform_arr)\n",
    "\n",
    "# Display the results for chip 2. The diagonal transformation matrix no longer\n",
    "# produces a result which visually matches the sRGB image.\n",
    "plot_color_scope(t_chip_2)\n",
    "display_image(t_chip_2)\n",
    "display_image(srgb_chip_2)\n",
    "display(transform_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c833647-7e60-46b6-949a-f574af9095e4",
   "metadata": {},
   "source": [
    "\n",
    "## Create a color conversion model using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fccb4-2cb0-47e6-9b2b-bd8c06068b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a layer with a trainiable 3x3 weights matrix which applies\n",
    "# this as the dot product to the input pixel.\n",
    "\n",
    "\n",
    "class ColorTransformLayer(layers.Layer):\n",
    "    def __init__(self, color_ch=3):\n",
    "        super(ColorTransformLayer, self).__init__()\n",
    "        self.color_ch = color_ch\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape), initializer=\"random_normal\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.tensordot(inputs, self.w, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d104e-fa6b-468e-a3a0-ca87a4d5d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model just consists of this, as that is all we want to do for this\n",
    "# experiment.\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        ColorTransformLayer(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd49da5-991e-458b-984c-a8c64a8a79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.build((3, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be8c36-e205-45b9-8651-ebdc7637e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = to_color_arr(exr_img_arr)\n",
    "y_data = to_color_arr(srgb_img_arr)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data)).shuffle(50000).batch(32)\n",
    "train_dataset, test_dataset = tf.keras.utils.split_dataset(dataset, left_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d083213-5801-468e-acd5-7ad7559a7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset.take(1):\n",
    "    display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc68797-3454-4de6-a26f-d7e414636690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=8, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9584f0-58da-41db-b8cc-f2ead5c7c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ff230-5592-41f7-8509-c598d116f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the predict method is *much* slower than just applying the dot product\n",
    "# from the array.\n",
    "\n",
    "\n",
    "def apply_model(img_arr, model):\n",
    "    r_orig, c_orig = img_arr.shape[:2]\n",
    "\n",
    "    output_arr = model.predict(to_color_arr(img_arr))\n",
    "    return output_arr.reshape((r_orig, c_orig, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052bbc17-3732-427f-acbf-ce99abe47c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_arr = model.layers[0].weights\n",
    "\n",
    "t_chip_0 = apply_color_transformation(raw_chip_0, transform_arr)\n",
    "\n",
    "# The scopes should be proportional, or at close if the training was successful.\n",
    "plot_color_scope(t_chip_0)\n",
    "plot_color_scope(srgb_chip_0)\n",
    "display_image(t_chip_0)\n",
    "display_image(srgb_chip_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628dc4a-b569-4090-b184-047b6151bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_chip_1 = apply_color_transformation(raw_chip_1, transform_arr)\n",
    "\n",
    "plot_color_scope(t_chip_1)\n",
    "plot_color_scope(srgb_chip_1)\n",
    "display_image(t_chip_1)\n",
    "display_image(srgb_chip_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fcb84-3b03-4cfd-9da6-f8a80aff6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the sample image.\n",
    "\n",
    "display_image(apply_model(exr_img_arr, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bbf9f-75b0-4929-85e3-4415b31d7cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(apply_color_transformation(exr_img_arr, transform_arr))\n",
    "display_image(srgb_img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851995ee-1891-406e-9d82-e6f2f37f7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An experiment with two color correction .\n",
    "\n",
    "dense_model = Sequential(\n",
    "    [\n",
    "        layers.Dense(3, activation=\"elu\"),\n",
    "        layers.Dense(3, activation=\"sigmoid\"),\n",
    "        ColorTransformLayer(),\n",
    "        # XyzToSrgbLayer(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dense_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "dense_model.build((3, 3))\n",
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffdb0e4-388d-4500-8da8-d5da8cbaad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model.fit(train_dataset, epochs=8, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f29449-2307-498c-8156-b323c4ae5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(apply_model(exr_img_arr, dense_model))\n",
    "display_image(srgb_img_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3f7c8-90aa-4f73-b588-185b562b3f82",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusions\n",
    "\n",
    "Using training data within a specific lighting scenario (ie. *indoor*) will\n",
    "produce a model which properly converts the color for that scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90bd54-5e43-43b1-ab60-aa1a9c89eebf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## Convert colors using the dcraw color matrix\n",
    "\n",
    "This roughly follows the process outlined here, with some questions and experimentation:\n",
    "\n",
    "https://www.numbercrunch.de/blog/2020/12/from-numbers-to-images-raw-image-processing-with-python/\n",
    "\n",
    "Using the approach outlined above would be the ideal approach, however, I was never able to achieve the same results through prior experimentation.\n",
    "\n",
    "### Outstanding Questions\n",
    "\n",
    "* Can white balance be applied after demosaicing? Conceiveably it can, as the pixel values still represent the same thing, both before and after conversion.\n",
    "\n",
    "* The approach above indicates that `rgb_xyz_matrix` is *the camera specific matrix that turns XYZ color into camera primaries* but given the name, it seems like this is the matrix which converts camera primaries into XYZ.\n",
    "\n",
    "* What would happen if we applied the `rgb_xyz_matrix` and `camera_whitebalance` to the image prior to training the model - would the result be generalized? What would the model weights look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d4cd7-283e-41a2-9e18-79c4a7ad8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_whitebalance(raw_path):\n",
    "    with rawpy.imread(raw_path) as raw_img:\n",
    "        camera_whitebalance = raw_img.camera_whitebalance\n",
    "        daylight_whitebalance = raw_img.daylight_whitebalance\n",
    "        rgb_xyz_matrix = raw_img.rgb_xyz_matrix\n",
    "\n",
    "    w_a = math.fsum(daylight_whitebalance) / math.fsum(camera_whitebalance)\n",
    "    cam_whitebalance = np.asarray(camera_whitebalance)[:3] * w_a\n",
    "    return cam_whitebalance, daylight_whitebalance, rgb_xyz_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1848e2-6ddc-4f0e-848a-cfc5b5fb1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rawpy.imread(RAW_PATH) as raw_img:\n",
    "    raw_img_array = raw_img.raw_image.astype(np.float32).copy()\n",
    "    camera_whitebalance = raw_img.camera_whitebalance\n",
    "    daylight_whitebalance = raw_img.daylight_whitebalance\n",
    "    rgb_xyz_matrix = raw_img.rgb_xyz_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a6426-61ef-4708-8f90-de30228ee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_whitebalance, _, _ = get_raw_whitebalance(RAW_PATH)\n",
    "camera_whitebalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d024cf-739c-465b-9871-3bbda1ed3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_matrix = rgb_xyz_matrix[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d8de4-7e3f-41bc-885f-885ad63e717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "srgb_to_xyz = np.array(\n",
    "    [\n",
    "        [0.4124564, 0.3575761, 0.1804375],\n",
    "        [0.2126729, 0.7151522, 0.0721750],\n",
    "        [0.0193339, 0.1191920, 0.9503041],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# This roughly matches the sRGB D65 matrix shown here:\n",
    "# http://www.brucelindbloom.com/index.html?Eqn_RGB_XYZ_Matrix.html\n",
    "xyz_to_srgb = np.linalg.inv(srgb_to_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016f334-2285-4f60-b87b-25f33a942039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this assumes the cam_matrix is xyz_to_cam\n",
    "srgb_to_cam = np.dot(cam_matrix, srgb_to_xyz)\n",
    "cam_to_srgb = np.linalg.inv(srgb_to_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a9213-03e3-4aac-b9c0-e08f23c0f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the difference between the dot product and the einsum?\n",
    "srgb_img_arr = np.einsum(\"ij,...j\", cam_to_srgb, cam_whitebalance * exr_img_arr)\n",
    "srgb_img_dot = np.dot(cam_whitebalance * exr_img_arr, cam_to_srgb)\n",
    "\n",
    "display_image(transform.normalize_arr(srgb_img_arr))\n",
    "display_image(transform.normalize_arr(srgb_img_dot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcab27d9-6a22-47c5-82a0-7190fbdb55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying this as the inverse. ie. if the cam_matrix is cam_to_xyz\n",
    "\n",
    "xyz_img_arr = np.einsum(\"ij,...j\", cam_matrix, exr_img_arr)\n",
    "xyz_img_dot = np.dot(exr_img_arr, cam_matrix)\n",
    "\n",
    "srgb_img_arr = np.einsum(\"ij,...j\", xyz_to_srgb, xyz_img_arr)\n",
    "srgb_img_dot = np.dot(xyz_img_arr, srgb_to_xyz)\n",
    "display_image(transform.normalize_arr(srgb_img_arr))\n",
    "display_image(transform.normalize_arr(srgb_img_dot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dafac5-38d4-4985-b0b7-5f61886e5887",
   "metadata": {},
   "source": [
    "### Conclusions?\n",
    "\n",
    "It is difficult to make a conclusion from the operation above other than there is something significant I don't understand in the operations above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
