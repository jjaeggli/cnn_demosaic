{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7586f6b0-541d-438d-81e0-8f31ec020f01",
   "metadata": {},
   "source": [
    "# Color-Levels Model\n",
    "\n",
    "This model intends to two additional concepts that were not present in the color model.\n",
    "\n",
    "* Overal image level parameters should be trained independently of the color curves so that the model is not specifically trained to translate from a color at a specific level, stauration, or intensity. (Although different \"looks\" may have different saturation curves at different levels.)\n",
    "* The model could be trained in such a way that the core color correction layers of a model could be frozen and aesthetic and color temperature parameters of an image can be left unfrozen to differentiate different styles or looks for an image. The parameters of these layers can be blended or merged to apply different types of color correction to an image.\n",
    "\n",
    "The challenge with both of these approaches is to isolate the fundamental parameters into specific layers while the aesthetic parameters are trained independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d5c92-2c9f-4558-ab12-53c0c4843120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pyexr\n",
    "import rawpy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Prevent TensorFlow from allocating all GPU memory.\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cnn_demosaic import exposure_model\n",
    "from cnn_demosaic import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644380dc-dc6c-4847-8964-226dd63d5cd1",
   "metadata": {},
   "source": [
    "## Configure common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89abbe-b3af-4440-9df1-663ad636a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    plt.imshow(image, vmin=0, vmax=1, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def to_color_arr(img_arr):\n",
    "    \"\"\"Flatten the 2D image to an array.\"\"\"\n",
    "    return img_arr.reshape((img_arr.size // 3, 3))\n",
    "\n",
    "\n",
    "def random_sampling(target_arr, n_samples=1000):\n",
    "    \"\"\"Returns an array containing n_samples from a color swatch.\"\"\"\n",
    "    rand_index = np.random.randint(0, target_arr.shape[0], n_samples)\n",
    "    return target_arr[rand_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1891bf4-142a-4a3f-b507-873cdb52430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_properties(raw_path):\n",
    "    with rawpy.imread(raw_path) as raw_img:\n",
    "        camera_whitebalance = raw_img.camera_whitebalance\n",
    "        daylight_whitebalance = np.asarray(raw_img.daylight_whitebalance)\n",
    "        rgb_xyz_matrix = raw_img.rgb_xyz_matrix[:3]\n",
    "\n",
    "    w_a = math.fsum(daylight_whitebalance) / math.fsum(camera_whitebalance)\n",
    "    cam_whitebalance = np.asarray(camera_whitebalance)[:3] * w_a\n",
    "    return cam_whitebalance, daylight_whitebalance, rgb_xyz_matrix\n",
    "\n",
    "\n",
    "def get_wb_params(paths):\n",
    "    for path in paths:\n",
    "        cam_whitebalance, _, _ = get_raw_properties(path)\n",
    "        yield cam_whitebalance\n",
    "\n",
    "\n",
    "# Usign the predict method is *much* slower than just applying the dot product\n",
    "# from the array.\n",
    "\n",
    "\n",
    "def apply_model(img_arr, model):\n",
    "    r_orig, c_orig = img_arr.shape[:2]\n",
    "\n",
    "    output_arr = model.predict(to_color_arr(img_arr))\n",
    "    return output_arr.reshape((r_orig, c_orig, 3))\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def tf_rgb_luma(rgb_ch):\n",
    "    # Convert the image to a luma channel using the following ratios:\n",
    "    # 0.299R + 0.587G + 0.114B\n",
    "    return rgb_ch[:, 0] * 0.299 + rgb_ch[:, 1] * 0.587 + rgb_ch[:, 2] * 0.114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4cc00-e787-4967-88b6-42c51c5b652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "srgb_to_xyz = np.array(\n",
    "    [\n",
    "        [0.4124564, 0.3575761, 0.1804375],\n",
    "        [0.2126729, 0.7151522, 0.0721750],\n",
    "        [0.0193339, 0.1191920, 0.9503041],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# This roughly matches the sRGB D65 matrix shown here:\n",
    "# http://www.brucelindbloom.com/index.html?Eqn_RGB_XYZ_Matrix.html\n",
    "xyz_to_srgb = np.linalg.inv(srgb_to_xyz)\n",
    "\n",
    "xyz_to_srgb_tf = tf.constant(xyz_to_srgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1112c-c664-415a-9edc-b9d45dbf17d2",
   "metadata": {},
   "source": [
    "## Set up dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147c849-9a6f-4fe7-a32a-2e95b6274159",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PREFIX = \"/media/jake/Media/datasets/fuji_raw/xe2/125_FUJI\"\n",
    "\n",
    "TRAINING_PATHS = [\n",
    "    (\"DSCF5752_card.exr\", \"DSCF5752_card.png\"),\n",
    "    (\"DSCF5759_card.exr\", \"DSCF5759_card.png\"),\n",
    "    (\"DSCF5760_card.exr\", \"DSCF5760_card.png\"),\n",
    "    (\"DSCF5761_card.exr\", \"DSCF5761_card.png\"),\n",
    "    (\"DSCF5731_card.exr\", \"DSCF5731_card_srgb.png\"),  # Outdoor, partly cloudy.\n",
    "    (\"DSCF5782.exr\", \"DSCF5782.JPG\"),\n",
    "    (\"DSCF5783.exr\", \"DSCF5783.JPG\"),\n",
    "    (\"DSCF5796.exr\", \"DSCF5796.JPG\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbde866-d170-4833-a615-1e4ab9eedc68",
   "metadata": {},
   "source": [
    "## Set up datasets for a model including WB params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac672fba-ad2f-4847-818e-d623139a7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pairs(a_arr, b_arr, n_samples=100000):\n",
    "    \"\"\"Returns an array containing n_samples from a color swatch.\"\"\"\n",
    "    assert len(a_arr) == len(b_arr)\n",
    "    rand_index = np.random.randint(0, a_arr.shape[0], n_samples)\n",
    "    return a_arr[rand_index], b_arr[rand_index]\n",
    "\n",
    "\n",
    "def load_dataset(feature_image_path, srgb_image_path):\n",
    "    feat_img_arr = pyexr.read(f\"{DATASET_PREFIX}/{feature_image_path}\")[:, :, :3]\n",
    "    srgb_img_arr = np.asarray(Image.open(f\"{DATASET_PREFIX}/{srgb_image_path}\"))[:, :, :3] / 255\n",
    "\n",
    "    feat_rgb_samples, targ_rgb_samples = sample_pairs(\n",
    "        to_color_arr(feat_img_arr), to_color_arr(srgb_img_arr), 128 * 128 * 128\n",
    "    )\n",
    "\n",
    "    # TODO(): Augment data by changing the quality of the exposure and with different images.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((feat_rgb_samples, targ_rgb_samples))\n",
    "    dataset = dataset.batch(128 * 128).map(lambda i, j: (i, tf_rgb_luma(j)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d3cc5-43f5-47d7-904e-1ce2525f562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_datasets = []\n",
    "\n",
    "for i in range(len(TRAINING_PATHS)):\n",
    "    levels_datasets.append(load_dataset(*TRAINING_PATHS[i]))\n",
    "\n",
    "levels_datasets_combined = tf.data.Dataset.sample_from_datasets(levels_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c45127-5882-438d-bee2-0db572faea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in levels_datasets_combined.take(1):\n",
    "    print(i.shape)\n",
    "    print(j.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee7d67-4509-4ed0-a404-01ec9d4f158a",
   "metadata": {},
   "source": [
    "## Define Correction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e7593-7b9a-42bd-b0e6-9ef037807a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogNormalizationLayer(layers.Layer):\n",
    "    def __init__(self, axis=1, epsilon=1e-6, **kwargs):\n",
    "        super(LogNormalizationLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # print(f\"LogNormalizationLayer input shape: {input_shape}\")\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def call(self, inputs):\n",
    "        log_inputs = tf.math.log(tf.cast(inputs, dtype=tf.float32) + self.epsilon)\n",
    "\n",
    "        max_log_input = tf.math.reduce_max(log_inputs, axis=self.axis, keepdims=True)\n",
    "        normalized_output = log_inputs / (max_log_input + self.epsilon)\n",
    "\n",
    "        return normalized_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(LogNormalizationLayer, self).get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"axis\": self.axis,\n",
    "                \"epsilon\": self.epsilon,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def levels_fn(img_arr, in_min, in_max):\n",
    "    dyn_range = in_max - in_min\n",
    "    dyn_range = dyn_range if dyn_range != 0.0 else 0.00001\n",
    "\n",
    "    scale_ratio = 1.0 / dyn_range\n",
    "    return (img_arr - in_min) * scale_ratio\n",
    "\n",
    "\n",
    "class LevelsLayer(layers.Layer):\n",
    "    def __init__(self, color_ch=3):\n",
    "        super(LevelsLayer, self).__init__()\n",
    "        self.color_ch = color_ch\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # print(f\"LevelsLayer input shape: {input_shape}\")\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Parameters list must be iterated / mapped to apply gamma exponent to\n",
    "        # each sub array in the first dimension.\n",
    "\n",
    "        @tf.function()\n",
    "        def apply_levels(parameters):\n",
    "            img_arr, weights = parameters\n",
    "            adjusted_image = levels_fn(img_arr, weights[0], weights[1])\n",
    "\n",
    "            # Dimensions of the input and output must match.\n",
    "            return [adjusted_image, weights]\n",
    "\n",
    "        return tf.map_fn(apply_levels, inputs)[0]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "\n",
    "class GammaAdjLayer(layers.Layer):\n",
    "    def __init__(self, color_ch=3):\n",
    "        super(GammaAdjLayer, self).__init__()\n",
    "        self.color_ch = color_ch\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # print(f\"GammaAdjLayer input shape: {input_shape}\")\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Parameters list must be iterated / mapped to apply gamma exponent to\n",
    "        # each sub array in the first dimension.\n",
    "\n",
    "        @tf.function()\n",
    "        def apply_pow(parameters):\n",
    "            img_arr, exp = parameters\n",
    "            # Dimensions of the input and output must match.\n",
    "            return [tf.pow(img_arr, exp), exp]\n",
    "\n",
    "        return tf.map_fn(apply_pow, inputs)[0]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def s_curve_fn(img_arr, offset, contrast, slope):\n",
    "    output = (img_arr + offset) * contrast\n",
    "    return 1 / (1 + tf.math.exp(slope * output))\n",
    "\n",
    "\n",
    "class ColorCurveAdjLayer(layers.Layer):\n",
    "    def __init__(self, color_ch=3):\n",
    "        super(ColorCurveAdjLayer, self).__init__()\n",
    "        self.color_ch = color_ch\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # print(f\"ColorCurveAdjLayer input shape: {input_shape}\")\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def call(self, inputs):\n",
    "        @tf.function()\n",
    "        def apply_curve(parameters):\n",
    "            rgb_arr, weights = parameters\n",
    "            # weights should contain 3 parameters.\n",
    "            output = s_curve_fn(rgb_arr, weights[0], weights[1], weights[2])\n",
    "            return [output, weights]\n",
    "\n",
    "        return tf.map_fn(apply_curve, inputs)[0]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "\n",
    "class LumaLayer(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(LumaLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def call(self, rgb_ch):\n",
    "        output_shape = (self.input_shape[1],)\n",
    "        y_ch = tf.map_fn(\n",
    "            tf_rgb_luma,\n",
    "            rgb_ch,\n",
    "            fn_output_signature=tf.TensorSpec(shape=output_shape, dtype=tf.float32),\n",
    "        )\n",
    "        return y_ch\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, input_shape[1])\n",
    "\n",
    "\n",
    "# Must disable jit_compile to avoid XLA graph compilation errors. But guess what...\n",
    "# Disabling jit compilation here has no effect. Disabling it globally for the model\n",
    "# however does.\n",
    "@tf.function(jit_compile=False)\n",
    "def tf_histogram(inputs, bins=16):\n",
    "    return tf.histogram_fixed_width(inputs, [0.0, 1.0], bins)\n",
    "\n",
    "\n",
    "class HistogramLayer(layers.Layer):\n",
    "    def __init__(self, bins=16):\n",
    "        super(HistogramLayer, self).__init__()\n",
    "        self.bins = bins\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def call(self, hist_input):\n",
    "        output = tf.map_fn(\n",
    "            lambda i: tf_histogram(i, self.bins),\n",
    "            hist_input,\n",
    "            fn_output_signature=tf.TensorSpec(shape=(self.bins,), dtype=tf.int32),\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(HistogramLayer, self).get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"bins\": self.bins,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4c229-9649-4e96-855e-7aca9cdfd265",
   "metadata": {},
   "source": [
    "### Model Strcuture\n",
    "\n",
    "The model inputs are RGB image pixel data and outputs are RGB pixel data with corrected luminance levels. For the purpose of this model, we don't want to pass in information about the target image levels, we want a generalized approach, which means that for any image we pass in, we get the \"typical image processing\" version of that image. The result may not be a LUT - ie. different input images may map to different curves based on the exposure. Right now, here's the theoretical process.\n",
    "\n",
    "* Input - RGB image array\n",
    "* Create variable containing luminance histogram\n",
    "* Sub Model - Compute Processing Parameters\n",
    "    - Input luminance histogram (16 buckets)\n",
    "    - Dense Layers\n",
    "    - Output parameters: (gamma, midpoint, contrast, slope)\n",
    "* Gamma layer - (gamma)\n",
    "* S-Curve Layer - (midpoint, contrast, slope)\n",
    "* Convert output to luma channel\n",
    "* Output luma channel\n",
    "\n",
    "Train against luma channel image training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e37628-757a-433b-9951-175480d5bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_model(exp_model, hist_bins=32):\n",
    "    \"\"\"This model is the training harness that allows the parameters model\n",
    "    to be trained on image data. It also generates nice grayscale images.\"\"\"\n",
    "    rgb_input = keras.Input(shape=(None, 3), name=\"rgb_input\")\n",
    "\n",
    "    rgb_layers = layers.Identity()(rgb_input)\n",
    "\n",
    "    # Get the Y channel histogram\n",
    "    y_ch = LumaLayer()(rgb_input)\n",
    "    y_hist = HistogramLayer(bins=hist_bins)(y_ch)\n",
    "\n",
    "    # Levels model: determine image processing parameters.\n",
    "    # if levels_model is None:\n",
    "    #     levels_model = correction_parameters_model(hist_size=hist_bins, dense_layers=dense_layers)\n",
    "\n",
    "    levels_weights, gamma_weights, curve_weights = exp_model(y_hist)\n",
    "\n",
    "    # Apply the predicted levels and curves to the output.\n",
    "    rgb_layers = GammaAdjLayer()([rgb_layers, gamma_weights])\n",
    "    rgb_layers = LevelsLayer()([rgb_layers, levels_weights])\n",
    "    rgb_layers = ColorCurveAdjLayer()([rgb_layers, curve_weights])\n",
    "\n",
    "    # Convert the output to luma for training comparison.\n",
    "    rgb_layers = LumaLayer()(rgb_layers)\n",
    "\n",
    "    model = keras.Model(inputs=[rgb_input], outputs=rgb_layers)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44105681-d333-45de-a138-86d0655735a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_model = exposure_model.exposure_params_model()\n",
    "test_model = create_test_model(exp_model)\n",
    "test_model.compile(optimizer=\"adam\", loss=\"mse\", jit_compile=False)\n",
    "test_model.build((None, 3))\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695d66a-1a65-4561-93b6-329e97e181dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = test_model.predict(np.random.rand(1, 128 * 128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4b2c4-b1f7-403a-9342-6ab27839fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = test_model.fit(levels_datasets_combined.batch(32), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb732c-7312-4f4c-9acc-ff9d01cc2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e8bd1-4c87-468b-8600-2ece25029638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "best_model = None\n",
    "best_exp_model = None\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 10\n",
    "num_runs = 20\n",
    "\n",
    "hyperparams = []\n",
    "\n",
    "for i_l in [16]:\n",
    "    for i_b in [32]:\n",
    "        hyperparams.append([i_l, i_b])\n",
    "\n",
    "\n",
    "for i_l, i_b in hyperparams:\n",
    "    for i in range(num_runs):\n",
    "        print(f\"Training model {i + 1}/{num_runs}: hist_bins: [{i_b}] dense_layers: [{i_l}]\")\n",
    "\n",
    "        exp_model = exposure_model.exposure_params_model(hist_size=i_b, dense_layers=i_l)\n",
    "        model = create_test_model(exp_model, hist_bins=i_b)\n",
    "\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\", jit_compile=False)\n",
    "        model.build((None, 3))\n",
    "\n",
    "        history = model.fit(levels_datasets_combined.batch(32), epochs=epochs, verbose=0)\n",
    "\n",
    "        # Get the validation loss. (change this when using validation data)\n",
    "        train_loss = history.history[\"loss\"][-1]\n",
    "        print(f\"Finished model {i + 1}/{num_runs}: {train_loss}\")\n",
    "\n",
    "        val_loss = train_loss\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "            best_exp_model = exp_model\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"hist_bins\": i_b,\n",
    "                \"dense_layers\": i_l,\n",
    "                \"train_loss\": train_loss,\n",
    "            }\n",
    "        )\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(f\"Best loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb5e01-66e6-407a-86ba-a84175ab0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.layers[2].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a08f92-9cbc-4abb-bce7-43141ea27014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform additional training on the best model to see if additional performance can be squeezed out of it\n",
    "best_model.fit(levels_datasets_combined.batch(32), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632dec60-c5a3-49d0-a0ed-1233cb2bed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results).groupby([\"hist_bins\", \"dense_layers\"])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for (hist_bins, dense_layers), group in results_df:\n",
    "    plt.scatter(\n",
    "        group[\"train_loss\"], group[\"dense_layers\"], label=f\"B={hist_bins}, L={dense_layers}\"\n",
    "    )\n",
    "\n",
    "# Set the x-axis to log scale\n",
    "plt.gca().set_xscale(\"log\")\n",
    "\n",
    "plt.xlabel(\"Training Loss\")\n",
    "plt.ylabel(\"Layers\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9db81-a8b7-4e03-9c49-66b607085bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_correction_model_predict(img_arr, model):\n",
    "    img_r, img_c = 256, 256\n",
    "\n",
    "    img_resized = tf.image.resize(img_arr, [img_r, img_c])\n",
    "    display_image(img_resized)\n",
    "    display(img_resized.shape)\n",
    "\n",
    "    img_linear_arr = tf.reshape(img_resized, (1, img_r * img_c, 3))\n",
    "    display(img_linear_arr.shape)\n",
    "\n",
    "    output_arr = model.predict(img_linear_arr)\n",
    "    display(output_arr.shape)\n",
    "\n",
    "    # Currently the output we're looking at is a 1 channel image.\n",
    "    return output_arr.reshape((img_r, img_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb28ccf-1311-47ca-85d4-ef210b511150",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exr_img_path = f\"{DATASET_PREFIX}/DSCF5782.exr\"\n",
    "test_exr_img_arr = pyexr.read(test_exr_img_path)[:, :, :3]\n",
    "\n",
    "output_img_predict = apply_correction_model_predict(test_exr_img_arr, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c30120-6fac-49dd-991d-0204f1e022d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(output_img_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cc926-df59-48b0-be59-4fd9c19d568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exr_img_path = f\"{DATASET_PREFIX}/DSCF5796.exr\"\n",
    "test_exr_img_arr = pyexr.read(test_exr_img_path)[:, :, :3]\n",
    "\n",
    "output_img_predict = apply_correction_model_predict(test_exr_img_arr, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b8327-01f7-42bc-82cd-29f9b38ce5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(output_img_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5851776-c213-451c-8aba-a6c436cb0d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyexr.write(\"test_output_levels_1.exr\", output_img_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403f27f-f24f-4971-b4ea-d98d2b0e0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_exp_model.save_weights(\"exp_model_32_16_0.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15515017-440d-4c75-ac0d-d61652396752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
