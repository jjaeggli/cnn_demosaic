{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d5c92-2c9f-4558-ab12-53c0c4843120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pyexr\n",
    "import rawpy\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cnn_demosaic import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e38b6-a9cd-4106-9702-6e53e1125592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Prevent TensorFlow from allocating all GPU memory.\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644380dc-dc6c-4847-8964-226dd63d5cd1",
   "metadata": {},
   "source": [
    "## Configure common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89abbe-b3af-4440-9df1-663ad636a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    plt.imshow(image, vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def to_color_arr(img_arr):\n",
    "    \"\"\"Flatten the 2D image to an array.\"\"\"\n",
    "    return img_arr.reshape((img_arr.size // 3, 3))\n",
    "\n",
    "\n",
    "def random_sampling(target_arr, n_samples=1000):\n",
    "    \"\"\"Returns an array containing n_samples from a color swatch.\"\"\"\n",
    "    rand_index = np.random.randint(0, target_arr.shape[0], n_samples)\n",
    "    return target_arr[rand_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1891bf4-142a-4a3f-b507-873cdb52430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_properties(raw_path):\n",
    "    with rawpy.imread(raw_path) as raw_img:\n",
    "        camera_whitebalance = raw_img.camera_whitebalance\n",
    "        daylight_whitebalance = np.asarray(raw_img.daylight_whitebalance)\n",
    "        rgb_xyz_matrix = raw_img.rgb_xyz_matrix[:3]\n",
    "\n",
    "    w_a = math.fsum(daylight_whitebalance) / math.fsum(camera_whitebalance)\n",
    "    cam_whitebalance = np.asarray(camera_whitebalance)[:3] * w_a\n",
    "    return cam_whitebalance, daylight_whitebalance, rgb_xyz_matrix\n",
    "\n",
    "\n",
    "def get_wb_params(paths):\n",
    "    for path in paths:\n",
    "        cam_whitebalance, _, _ = get_raw_properties(path)\n",
    "        yield cam_whitebalance\n",
    "\n",
    "\n",
    "# Usign the predict method is *much* slower than just applying the dot product\n",
    "# from the array.\n",
    "\n",
    "\n",
    "def apply_model(img_arr, model):\n",
    "    r_orig, c_orig = img_arr.shape[:2]\n",
    "\n",
    "    output_arr = model.predict(to_color_arr(img_arr), batch_size=8192)\n",
    "    return output_arr.reshape((r_orig, c_orig, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4cc00-e787-4967-88b6-42c51c5b652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "srgb_to_xyz = np.array(\n",
    "    [\n",
    "        [0.4124564, 0.3575761, 0.1804375],\n",
    "        [0.2126729, 0.7151522, 0.0721750],\n",
    "        [0.0193339, 0.1191920, 0.9503041],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# This roughly matches the sRGB D65 matrix shown here:\n",
    "# http://www.brucelindbloom.com/index.html?Eqn_RGB_XYZ_Matrix.html\n",
    "xyz_to_srgb = np.linalg.inv(srgb_to_xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1112c-c664-415a-9edc-b9d45dbf17d2",
   "metadata": {},
   "source": [
    "## Set up datasets\n",
    "\n",
    "### Creating datasets\n",
    "\n",
    "Images are used as raw training inputs for images. The dataset is defined as input and output image pairs with a RAW image containing the as-shot parameters for the image. The image pairs correspond to the floating point demosaiced image (X) and the sRGB camera color corrected output image (y). Depending on the model, the white balance information from the camera may be used as an input. There are a number of considerations for the image chosen and their preparation. Each pixel in the output image should correspond to each pixel in the input image. The model will be evaluated on the ability to predict the output value of the pixel based on the input value, using color transformations. This means that lens and geometric transformations on the output image will move pixels from their original location. If using a color card, one approach is to slice the image into tiles so that all the pixels in one tile correspond to all the pixels in another tile of the same color, and to trim the image into this essential shape.\n",
    "\n",
    "An easier approach is to disable lens correction in the camera if possible, or use a manual lens which will does not provide information to the camera for image processing. Adjustments such as pincussion and barrel distortion correction will not be saved to the image, and each input pixel should correspond to each output pixel. One caveat to this is that the output EXR image is not yet cropped identically to the output JPG from the camera. Therefore manual alignment and cropping is currently required.\n",
    "\n",
    "Each input and output image is then sampled (100k pixel pairs) from each image. Distribution of color and lighting in the image affects the result. Using a wide variety of image and lighting situations when training a model will produce output with dull colors. Using a very similar set of images will produce a model that generates more accurate, vivid colors. Experimentation is important. Levels in the image are not currently normalized, and this could have a big impact on the output.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147c849-9a6f-4fe7-a32a-2e95b6274159",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PREFIX = \"/media/jake/Media/datasets/fuji_raw/xe2/125_FUJI\"\n",
    "\n",
    "# Cherrypick training images to batch similar results. These should really be\n",
    "# clustered by white balance parameters.\n",
    "\n",
    "RAW_PATHS = [\n",
    "    \"DSCF5752.RAF\",\n",
    "    \"DSCF5759.RAF\",\n",
    "    # \"DSCF5760.RAF\",\n",
    "    \"DSCF5761.RAF\",\n",
    "    # \"DSCF5731.RAF\",\n",
    "    # \"DSCF5782.RAF\",\n",
    "    # \"DSCF5783.RAF\",\n",
    "    \"DSCF5796.RAF\",\n",
    "]\n",
    "\n",
    "TRAINING_PATHS = [\n",
    "    (\"DSCF5752_card.exr\", \"DSCF5752_card.png\"),\n",
    "    (\"DSCF5759_card.exr\", \"DSCF5759_card.png\"),\n",
    "    # (\"DSCF5760_card.exr\", \"DSCF5760_card.png\"),\n",
    "    (\"DSCF5761_card.exr\", \"DSCF5761_card.png\"),\n",
    "    # (\"DSCF5731_card.exr\", \"DSCF5731_card_srgb.png\"),  # Outdoor, partly cloudy.\n",
    "    # (\"DSCF5782.exr\", \"DSCF5782.JPG\"),\n",
    "    # (\"DSCF5783.exr\", \"DSCF5783.JPG\"),\n",
    "    (\"DSCF5796.exr\", \"DSCF5796.JPG\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f289e2-d193-498e-84f2-a4f07f75f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_demosaic import exposure_model\n",
    "from cnn_demosaic import exposure\n",
    "\n",
    "exp_model = exposure_model.create_exposure_model('./params_model_32_16_0.weights.h5')\n",
    "exp = exposure.Exposure(exp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359ceddf-5b8e-481f-9b15-d1fef7e377e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pairs(a_arr, b_arr, n_samples=128*128):\n",
    "    \"\"\"Returns an array containing n_samples from a color swatch.\"\"\"\n",
    "    assert len(a_arr) == len(b_arr)\n",
    "    rand_index = np.random.randint(0, a_arr.shape[0], n_samples)\n",
    "    return a_arr[rand_index], b_arr[rand_index]\n",
    "\n",
    "\n",
    "def load_dataset(feature_image_path, srgb_image_path, wb_array=None):\n",
    "    feat_img_arr = pyexr.read(f\"{DATASET_PREFIX}/{feature_image_path}\")[:, :, :3]\n",
    "    srgb_img_arr = np.asarray(Image.open(f\"{DATASET_PREFIX}/{srgb_image_path}\"))[:, :, :3] / 255\n",
    "\n",
    "    # Apply the levels model to the image.\n",
    "    proc_img_arr = np.asarray(exp.process(feat_img_arr))\n",
    "    \n",
    "    feat_rgb_samples, targ_rgb_samples = sample_pairs(\n",
    "        to_color_arr(proc_img_arr), to_color_arr(srgb_img_arr)\n",
    "    )\n",
    "\n",
    "    if wb_array is not None:\n",
    "        wb_matrix_full = np.full_like(feat_rgb_samples, wb_array)\n",
    "        feat_rgb_samples = np.concatenate((feat_rgb_samples, wb_matrix_full), axis=1)\n",
    "\n",
    "    # display(f'loaded dataset shape: {feat_rgb_samples.shape}')\n",
    "    return tf.data.Dataset.from_tensor_slices((feat_rgb_samples, targ_rgb_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4c230-94a1-4cae-bed2-48673999fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def normalize_dyn_range(array_input):\n",
    "    dyn_range = 0.98\n",
    "    offset = (1.0 - dyn_range) / 2.0\n",
    "    arr_min = tf.math.reduce_min(array_input)\n",
    "    arr_max = tf.math.reduce_max(array_input)\n",
    "    denom = tf.math.maximum(arr_max - arr_min, 0.001)\n",
    "    return (array_input - arr_min) * dyn_range / denom + offset\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def normalize_batches(X_inp, y_inp):\n",
    "    X_rgb = normalize_dyn_range(X_inp[:, 0:3])\n",
    "    X_out = tf.concat([X_rgb, X_inp[:, 3:]], 1)\n",
    "    y_out = normalize_dyn_range(y_inp)\n",
    "    return X_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756a371-c9c4-4008-a130-f0cee2b501c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads datasets combined with whitebalance parameters from RAW_PATHS and TRAINING_PATHS\n",
    "full_raw_paths = [f\"{DATASET_PREFIX}/{p}\" for p in RAW_PATHS]\n",
    "wb_params = list(get_wb_params(full_raw_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0280592-c910-4014-92e8-83b087354f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads datasets combined with whitebalance parameters from RAW_PATHS and TRAINING_PATHS\n",
    "full_raw_paths = [f\"{DATASET_PREFIX}/{p}\" for p in RAW_PATHS]\n",
    "wb_params = list(get_wb_params(full_raw_paths))\n",
    "\n",
    "wb_datasets = []\n",
    "\n",
    "for i in range(len(TRAINING_PATHS)):\n",
    "    # DEFAULT: No normalization of batches. Produces good results for constistent levels across training images.\n",
    "    # wb_datasets.append(load_dataset(*TRAINING_PATHS[i], wb_params[i]).batch(32))\n",
    "    # EXPERIMENT: Normalize image input levels so the model is not performing the bulk of the adjustment.\n",
    "    # Normalizing the dynamic range increases the overall loss but improves color accuracy.\n",
    "    wb_datasets.append(\n",
    "        load_dataset(*TRAINING_PATHS[i], wb_params[i]).batch(32).map(normalize_batches)\n",
    "    )\n",
    "\n",
    "wb_datasets_combined = tf.data.Dataset.sample_from_datasets(wb_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e7593-7b9a-42bd-b0e6-9ef037807a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraCorrectionLayer(layers.Layer):\n",
    "    def __init__(self, camera_matrix):\n",
    "        super(CameraCorrectionLayer, self).__init__()\n",
    "        self.camera_matrix = tf.convert_to_tensor(np.linalg.inv(camera_matrix))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        return\n",
    "        # if input_shape[1] != 3:\n",
    "        #     raise ValueError(f'input shape is not valid for this layer: {input_shape}')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.tensordot(inputs, self.camera_matrix, 1)\n",
    "\n",
    "\n",
    "class XyzToSrgbLayer(layers.Layer):\n",
    "    def __init__(self, color_ch=3):\n",
    "        super(XyzToSrgbLayer, self).__init__()\n",
    "        self.color_ch = color_ch\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        return\n",
    "        # if input_shape[1] != 3:\n",
    "        #     raise ValueError(f'input shape is not valid for this layer: {input_shape}')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.tensordot(inputs, xyz_to_srgb, 1)\n",
    "\n",
    "\n",
    "class ColorTransformLayer(layers.Layer):\n",
    "    def __init__(self, color_ch=3):\n",
    "        super(ColorTransformLayer, self).__init__()\n",
    "        self.color_ch = color_ch\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(3, 3), initializer=\"random_normal\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.tensordot(inputs, self.w, 1)\n",
    "\n",
    "\n",
    "class MultiColorTransformLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MultiColorTransformLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(3, 3), initializer=\"random_normal\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        @tf.function()\n",
    "        def apply_fn(inputs):\n",
    "            return tf.tensordot(inputs, self.w, 1)\n",
    "        \n",
    "        return tf.map_fn(\n",
    "            apply_fn,\n",
    "            inputs\n",
    "        )\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def tf_s_curve_fn(img_arr, offset, contrast, slope):\n",
    "    \"\"\"Applies the s-curve function in a TensorFlow context.\"\"\"\n",
    "    output = (img_arr + offset) * contrast\n",
    "    return 1 / (1 + tf.math.exp(slope * output))\n",
    "\n",
    "\n",
    "# Applies a channel independent s-curve to the image.\n",
    "class ColorCurveAdjLayer(layers.Layer):\n",
    "    def __init__(self, color_ch=3):\n",
    "        super(ColorCurveAdjLayer, self).__init__()\n",
    "        self.color_ch = color_ch\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.w = self.add_weight(shape=(3, 3), initializer=\"random_normal\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        @tf.function()\n",
    "        def apply_curve(rgb_arr):\n",
    "            # weights should contain 3 parameters.\n",
    "            output = tf_s_curve_fn(rgb_arr, self.w[0], self.w[1], self.w[2])\n",
    "            return output\n",
    "\n",
    "        return apply_curve(inputs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbde866-d170-4833-a615-1e4ab9eedc68",
   "metadata": {},
   "source": [
    "## Set up datasets for a model including WB params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee7d67-4509-4ed0-a404-01ec9d4f158a",
   "metadata": {},
   "source": [
    "## Define Correction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96fe13c-8daf-46b5-992b-308bb9facd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():   \n",
    "    # Merged input contains RGB values for the pixel as [0:3] and white balance\n",
    "    # values for the pixel as [3:]. The purpose of this approach is to create\n",
    "    # a general model which can color correct for images of any light source,\n",
    "    # however it doesn't seem to be working as intended, even though it is\n",
    "    # doing *something*.\n",
    "    merged_input = keras.Input(shape=(6,), name=\"merged_input\")\n",
    "    rgb_input = layers.Lambda(lambda x: x[:, 0:3])(merged_input)\n",
    "    wb_input = layers.Lambda(lambda x: x[:, 3:])(merged_input)\n",
    "\n",
    "    # Apply a set of weights to the white balance. The first white balance weights\n",
    "    # will be used on the camera rgb input. The second will be used on the\n",
    "    # converted RGB input.\n",
    "    wb_transform = layers.Dense(6)(wb_input)\n",
    "    wb_transform = layers.Dense(6)(wb_transform)\n",
    "    wb_transform = layers.Dense(6)(wb_transform)\n",
    "\n",
    "    # wb_transform = layers.Dense(3, activation=\"sigmoid\")(wb_transform)\n",
    "    wb_transform_1 = layers.Dense(3)(wb_transform)\n",
    "    wb_transform_1 = layers.Dense(3, activation=\"sigmoid\")(wb_transform_1)\n",
    "    wb_transform_1 = ColorTransformLayer()(wb_transform_1)\n",
    "\n",
    "    wb_transform_2 = layers.Dense(6)(wb_transform)\n",
    "    wb_transform_2 = layers.Dense(6)(wb_transform_2)\n",
    "    wb_transform_2 = layers.Dense(3)(wb_transform_2)\n",
    "    wb_transform_2 = layers.Dense(3, activation=\"sigmoid\")(wb_transform_2)\n",
    "    wb_transform_2 = ColorTransformLayer()(wb_transform_2)    \n",
    "\n",
    "    rgb_layers = layers.Identity()(rgb_input)\n",
    "\n",
    "    # Apply a transformed white balance.\n",
    "    rgb_layers = layers.Multiply()([rgb_layers, wb_transform_1])\n",
    "\n",
    "    # Apply per channel color curves to the input.\n",
    "    rgb_layers = ColorCurveAdjLayer()(rgb_layers)\n",
    "\n",
    "    # Apply the color transformation to the RGB image.\n",
    "    rgb_layers = ColorTransformLayer()(rgb_layers)\n",
    "    rgb_layers = XyzToSrgbLayer()(rgb_layers)\n",
    "\n",
    "    # Apply a second RGB color curve to the input.\n",
    "    rgb_layers = ColorCurveAdjLayer()(rgb_layers)\n",
    "\n",
    "    # EXP: previous best, this enabled!!\n",
    "    rgb_layers = layers.Multiply()([rgb_layers, wb_transform_2])\n",
    "    \n",
    "    # Apply a second transformation of the white balance to the output.\n",
    "    # This has a minimal but measurable improvement.\n",
    "    rgb_layers = layers.Multiply()([rgb_layers, wb_transform_2])\n",
    "\n",
    "    model = keras.Model(inputs=merged_input, outputs=rgb_layers)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e8ade-1c16-4fc8-a626-7153da69d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "once = True\n",
    "\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# I actually don't know if this loss function is useful.\n",
    "def level_loss(y_true, y_pred):\n",
    "    sq_diff = tf.square(y_true - y_pred)\n",
    "    sum_sq_diff = tf.reduce_sum(sq_diff, axis=-1)\n",
    "    error = tf.sqrt(sum_sq_diff)\n",
    "    return error\n",
    "\n",
    "def color_relative_loss(y_true, y_pred):\n",
    "    sum_true = tf.reshape(tf.reduce_sum(y_true, axis=-1), (-1, 1))\n",
    "    sum_pred = tf.reshape(tf.reduce_sum(y_pred, axis=-1), (-1, 1))\n",
    "    weighted = tf.square(y_true/sum_true - y_pred/sum_pred)\n",
    "    error = tf.reduce_sum(weighted, axis=-1)\n",
    "    return error\n",
    "\n",
    "def balanced_relative_loss(y_true, y_pred):\n",
    "    return mse_loss.call(y_true, y_pred) + color_relative_loss(y_true, y_pred)/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f40aff-1362-4993-826f-0015d357a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test = np.asarray([[1.0, 1.0, 1.0],\n",
    "                          [0.3, 0.1, 0.2]])\n",
    "y_pred_test = np.asarray([[0.5, 0.5, 0.5],\n",
    "                          [0.6, 0.2, 0.4]])\n",
    "\n",
    "display(color_relative_loss(y_true_test, y_pred_test))\n",
    "display(level_loss(y_true_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44105681-d333-45de-a138-86d0655735a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_correction_model = create_model()\n",
    "color_correction_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=balanced_relative_loss, metrics=['mse'])\n",
    "color_correction_model.build((None, 6))\n",
    "color_correction_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86087fe-043f-4da4-9099-44b643517dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695d66a-1a65-4561-93b6-329e97e181dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "best_model = None\n",
    "best_loss = float(\"inf\")\n",
    "epochs = 10\n",
    "num_runs = 20\n",
    "\n",
    "hyperparams = [1]\n",
    "\n",
    "\n",
    "for h in hyperparams:\n",
    "    for i in range(num_runs):\n",
    "        print(f\"Training model {i + 1}/{num_runs}.\")\n",
    "\n",
    "        model = create_model()\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=balanced_relative_loss, metrics=['mse'])\n",
    "        model.build((None, 6))\n",
    "\n",
    "        history = model.fit(wb_datasets_combined, epochs=epochs, verbose=0)\n",
    "\n",
    "        # Get the validation loss. (change this when using validation data)\n",
    "        train_loss = history.history[\"loss\"][-1]\n",
    "        print(f\"Finished model {i + 1}/{num_runs}: {train_loss}\")\n",
    "\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            best_model = model\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"train_loss\": train_loss,\n",
    "            }\n",
    "        )\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(f\"Best loss: {best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee4a9f-bc70-44b9-83b0-0f2e36766978",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(wb_datasets_combined, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8687c5-ebbc-4ce8-a7c7-698078fc3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_correction_model.save_weights(f\"color_correction_model_0_7.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9db81-a8b7-4e03-9c49-66b607085bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_correction_model_predict(img_arr, model, wb_matrix):\n",
    "    r_orig, c_orig = img_arr.shape[:2]\n",
    "\n",
    "    img_linear_arr = to_color_arr(np.asarray(img_arr))\n",
    "\n",
    "    # Creates a whitebalance array and merges this with the image.\n",
    "    wb_matrix_full = np.full_like(img_linear_arr, wb_matrix)\n",
    "    process_arr = np.concatenate((img_linear_arr, wb_matrix_full), axis=1)\n",
    "    display(process_arr.shape)\n",
    "\n",
    "    output_arr = model.predict(process_arr, batch_size=8192)\n",
    "    return output_arr.reshape((r_orig, c_orig, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb28ccf-1311-47ca-85d4-ef210b511150",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exr_img_path = f\"{DATASET_PREFIX}/DSCF5796.exr\"\n",
    "test_raw_img_path = f\"{DATASET_PREFIX}/DSCF5796.RAF\"\n",
    "# test_exr_img_path = f\"{DATASET_PREFIX}/DSCF5783.exr\"\n",
    "# test_raw_img_path = f\"{DATASET_PREFIX}/DSCF5783.RAF\"\n",
    "\n",
    "test_exr_img_arr = exp.process(pyexr.read(test_exr_img_path)[:, :, :3])\n",
    "test_wb, _, _ = get_raw_properties(test_raw_img_path)\n",
    "\n",
    "output_img = apply_correction_model_predict(test_exr_img_arr, best_model, test_wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c30120-6fac-49dd-991d-0204f1e022d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661d5b8-a27a-4e50-b281-dda3548607c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason DarkTable is having problems with this EXR. Gimp will produce\n",
    "# usable results when using the eyedropper tool on an image to adjust color\n",
    "# curves, but this is really what we're trying to avoid doing.\n",
    "\n",
    "pyexr.write(\n",
    "    f\"./corrected_output_img_4.exr\",\n",
    "    np.asarray(output_img),\n",
    "    precision=pyexr.HALF,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e12812-77d4-41c9-b87f-7d8098f341e5",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This approach above is still not working well. I suspect that the model is not\n",
    "using the white balancing layers, which is why it is producing the muddy\n",
    "results, which seem to be too warm or too cool depending on which lighting\n",
    "scenario the images were taken with.\n",
    "\n",
    "### Alternative approaches\n",
    "\n",
    "* Create a color correction model which we co-train to produce a white balance\n",
    "  transform, using as input either:\n",
    "    - the white / black point\n",
    "    - the white balance type\n",
    "    - color correction weights\n",
    "* Create a bunch of color correction sample images using color temperature controlled panel lights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403f27f-f24f-4971-b4ea-d98d2b0e0132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
