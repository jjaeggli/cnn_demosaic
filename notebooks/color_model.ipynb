{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d5c92-2c9f-4558-ab12-53c0c4843120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pyexr\n",
    "import rawpy\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from cnn_demosaic import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e38b6-a9cd-4106-9702-6e53e1125592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Prevent TensorFlow from allocating all GPU memory.\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644380dc-dc6c-4847-8964-226dd63d5cd1",
   "metadata": {},
   "source": [
    "## Configure common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89abbe-b3af-4440-9df1-663ad636a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_image(image):\n",
    "    plt.imshow(image, vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "\n",
    "def to_color_arr(img_arr):\n",
    "    \"\"\"Flatten the 2D image to an array.\"\"\"\n",
    "    return img_arr.reshape((img_arr.size // 3, 3))\n",
    "\n",
    "def random_sampling(target_arr, n_samples=1000):\n",
    "    \"\"\"Returns an array containing n_samples from a color swatch.\"\"\"\n",
    "    rand_index = np.random.randint(0, target_arr.shape[0], n_samples)\n",
    "    return target_arr[rand_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1891bf4-142a-4a3f-b507-873cdb52430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_raw_properties(raw_path):\n",
    "    with rawpy.imread(raw_path) as raw_img:\n",
    "        camera_whitebalance = raw_img.camera_whitebalance\n",
    "        daylight_whitebalance = np.asarray(raw_img.daylight_whitebalance)\n",
    "        rgb_xyz_matrix = raw_img.rgb_xyz_matrix[:3]\n",
    "\n",
    "    w_a = math.fsum(daylight_whitebalance) / math.fsum(camera_whitebalance)\n",
    "    cam_whitebalance = np.asarray(camera_whitebalance)[:3] * w_a\n",
    "    return cam_whitebalance, daylight_whitebalance, rgb_xyz_matrix\n",
    "\n",
    "def get_wb_params(paths):\n",
    "    for path in paths:\n",
    "        cam_whitebalance, _, _ = get_raw_properties(path)\n",
    "        yield cam_whitebalance\n",
    "\n",
    "# Usign the predict method is *much* slower than just applying the dot product\n",
    "# from the array.\n",
    "\n",
    "def apply_model(img_arr, model):\n",
    "    r_orig, c_orig = img_arr.shape[:2]\n",
    "    \n",
    "    output_arr = model.predict(to_color_arr(img_arr))\n",
    "    return output_arr.reshape((r_orig, c_orig, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4cc00-e787-4967-88b6-42c51c5b652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "srgb_to_xyz = np.array(\n",
    "    [\n",
    "        [0.4124564, 0.3575761, 0.1804375],\n",
    "        [0.2126729, 0.7151522, 0.0721750],\n",
    "        [0.0193339, 0.1191920, 0.9503041],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# This roughly matches the sRGB D65 matrix shown here:\n",
    "# http://www.brucelindbloom.com/index.html?Eqn_RGB_XYZ_Matrix.html\n",
    "xyz_to_srgb = np.linalg.inv(srgb_to_xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1112c-c664-415a-9edc-b9d45dbf17d2",
   "metadata": {},
   "source": [
    "## Set up datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147c849-9a6f-4fe7-a32a-2e95b6274159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the camera processed color card\n",
    "SRGB_PATH = '../training/DSCF5652_card_srgb.exr'\n",
    "# The path to the demosaiced but not color corrected color card\n",
    "EXR_PATH = '../training/DSCF5652_card.exr'\n",
    "# The path to the raw image\n",
    "RAW_PATH = '../training/DSCF5652.RAF'\n",
    "\n",
    "TEST_SRGB_PATH = '../training/DSCF5731_card_srgb.exr'\n",
    "TEST_EXR_PATH = '../training/DSCF5731_card.exr'\n",
    "TEST_RAW_PATH = '../training/DSCF5731.RAF'\n",
    "\n",
    "TEST_FULL_EXR_PATH = '../training/DSCF5731.exr'\n",
    "\n",
    "RAW_PATHS = [\n",
    "    '/media/jake/Media/datasets/fuji_raw/xe2/125_FUJI/DSCF5652.RAF',\n",
    "    '/media/jake/Media/datasets/fuji_raw/xe2/125_FUJI/DSCF5711.RAF',\n",
    "    '/media/jake/Media/datasets/fuji_raw/xe2/125_FUJI/DSCF5731.RAF']\n",
    "\n",
    "TRAINING_PATHS = [\n",
    "    ('../training/DSCF5652_card.exr', '../training/DSCF5652_card_srgb.exr'),\n",
    "    ('../training/DSCF5711_card.exr', '../training/DSCF5711_card_srgb.exr'),\n",
    "    ('../training/DSCF5731_card.exr', '../training/DSCF5731_card_srgb.exr'),   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf38b2-8492-433f-935f-4f11f4250bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "srgb_img_arr = pyexr.read(SRGB_PATH)[:,:,:3]\n",
    "exr_img_arr = pyexr.read(EXR_PATH)[:,:,:3]\n",
    "\n",
    "test_exr_img_arr = pyexr.read(TEST_EXR_PATH)[:,:,:3]\n",
    "test_srgb_img_arr = pyexr.read(TEST_SRGB_PATH)[:,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754bd47-7a34-4496-a675-f97a7e545a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indoor = to_color_arr(exr_img_arr)\n",
    "y_indoor = to_color_arr(srgb_img_arr)\n",
    "\n",
    "X_outdoor = to_color_arr(test_exr_img_arr)\n",
    "y_outdoor = to_color_arr(test_srgb_img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b0f5f-512f-4726-859c-63b33475669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_indoor = tf.data.Dataset.from_tensor_slices((X_indoor, y_indoor)).shuffle(1000)\n",
    "dataset_outdoor = tf.data.Dataset.from_tensor_slices((X_indoor, y_indoor)).shuffle(1000)\n",
    "\n",
    "dataset_combined = tf.data.Dataset.sample_from_datasets(\n",
    "    [dataset_indoor, dataset_outdoor], weights=[0.5, 0.5]).batch(32)\n",
    "\n",
    "train_dataset, test_dataset = tf.keras.utils.split_dataset(dataset_combined, left_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e7593-7b9a-42bd-b0e6-9ef037807a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CameraCorrectionLayer(layers.Layer):\n",
    "    def __init__(self, camera_matrix):\n",
    "        super(CameraCorrectionLayer, self).__init__()\n",
    "        self.camera_matrix = tf.convert_to_tensor(np.linalg.inv(camera_matrix))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        return\n",
    "        # if input_shape[1] != 3:\n",
    "        #     raise ValueError(f'input shape is not valid for this layer: {input_shape}')\n",
    "    def call(self, inputs):\n",
    "        return tf.tensordot(inputs, self.camera_matrix, 1)\n",
    "\n",
    "class XyzToSrgbLayer(layers.Layer):\n",
    "    def __init__(self, color_ch=3):\n",
    "        super(XyzToSrgbLayer, self).__init__()\n",
    "        self.color_ch = color_ch\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        return\n",
    "        # if input_shape[1] != 3:\n",
    "        #     raise ValueError(f'input shape is not valid for this layer: {input_shape}')\n",
    "    def call(self, inputs):\n",
    "        return tf.tensordot(inputs, xyz_to_srgb, 1)\n",
    "\n",
    "class ColorTransformLayer(layers.Layer):\n",
    "    def __init__(self, color_ch=3):\n",
    "        super(ColorTransformLayer, self).__init__()\n",
    "        self.color_ch = color_ch\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(3,3),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "    def call(self, inputs):\n",
    "        return tf.tensordot(inputs, self.w, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fee386-0bfb-4a90-83af-a28dd1e9c2a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c82072-df1b-4851-b56f-e83eef41990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a model using the color correction layer for the fuji XE2.\n",
    "# This model will only fit one ligthing scenario.\n",
    "\n",
    "corrected_model = Sequential([\n",
    "    keras.Input(shape=(3,), batch_size=32),\n",
    "    # CameraCorrectionLayer(xe2_rgb_matrix),\n",
    "    layers.Dense(3, activation='elu'),\n",
    "    layers.Dense(3, activation='sigmoid'),\n",
    "    ColorTransformLayer(),\n",
    "    XyzToSrgbLayer(),\n",
    "])\n",
    "\n",
    "corrected_model.compile(optimizer='adam', loss='mse')\n",
    "corrected_model.build()\n",
    "corrected_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e44b4-4393-4f7e-957b-b6ab28a051e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_model.fit(train_dataset, epochs=8, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc034ab-3b8e-4ae4-8495-4133bbb7768f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Evaluate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80568c-6d50-4888-b64a-575086db3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RAW_PATH = '../training/DSCF5731.RAF'\n",
    "TEST_EXR_PATH = '../training/DSCF5731.exr'\n",
    "pt_cloudy_wb, _, xe2_rgb_matrix = get_raw(TEST_RAW_PATH)\n",
    "test_exr_img_arr = pyexr.read(TEST_EXR_PATH)[:,:,:3]\n",
    "\n",
    "display_image(apply_model(test_exr_img_arr * pt_cloudy_wb, corrected_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b91056-728d-408e-a175-c5afafa5d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(apply_model(test_exr_img_arr * pt_cloudy_wb, corrected_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbde866-d170-4833-a615-1e4ab9eedc68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Set up datasets for a model including WB params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac672fba-ad2f-4847-818e-d623139a7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pairs(a_arr, b_arr, n_samples=100000):\n",
    "    \"\"\"Returns an array containing n_samples from a color swatch.\"\"\"\n",
    "    assert len(a_arr) == len(b_arr)\n",
    "    rand_index = np.random.randint(0, a_arr.shape[0], n_samples)\n",
    "    return a_arr[rand_index], b_arr[rand_index]\n",
    "\n",
    "\n",
    "def load_dataset(feature_image_path, target_image_path, wb_array):\n",
    "    feat_img_arr = pyexr.read(feature_image_path)[:,:,:3]\n",
    "    targ_img_arr = pyexr.read(feature_image_path)[:,:,:3]\n",
    "\n",
    "    feat_rgb_samples, targ_rgb_samples = sample_pairs(to_color_arr(feat_img_arr), to_color_arr(targ_img_arr))\n",
    "    feat_rgb_with_wb = [np.concatenate((rgb, wb_array)) for rgb in feat_rgb_samples]\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((feat_rgb_with_wb, targ_rgb_samples))\n",
    "\n",
    "\n",
    "wb_params = list(get_wb_params(RAW_PATHS))\n",
    "\n",
    "wb_datasets = []\n",
    "\n",
    "for i in range(len(TRAINING_PATHS)):\n",
    "    wb_datasets.append(load_dataset(*TRAINING_PATHS[i], wb_params[i]))\n",
    "\n",
    "wb_datasets_combined = tf.data.Dataset.sample_from_datasets(wb_datasets, weights=[0.4, 0.3, 0.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee7d67-4509-4ed0-a404-01ec9d4f158a",
   "metadata": {},
   "source": [
    "## Define Correction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96fe13c-8daf-46b5-992b-308bb9facd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    # Merged input contains RGB values for the pixel as [0:3] and white balance\n",
    "    # values for the pixel as [3:]. The purpose of this approach is to create\n",
    "    # a general model which can color correct for images of any light source,\n",
    "    # however it doesn't seem to be working as intended, even though it is\n",
    "    # doing *something*.\n",
    "    merged_input = keras.Input(shape=(6,), name='merged_input')\n",
    "    rgb_input = layers.Lambda(lambda x: x[:, 0:3])(merged_input)\n",
    "    wb_input = layers.Lambda(lambda x: x[:, 3:])(merged_input)\n",
    "\n",
    "    # Apply a set of weights to the white balance. The first white balance weights\n",
    "    # will be used on the camera rgb input. The second will be used on the \n",
    "    # converted RGB input.\n",
    "    wb_transform_1 = ColorTransformLayer()(wb_input)\n",
    "    wb_transform_1 = layers.Dense(3)(wb_transform_1)\n",
    "    wb_transform_1 = layers.Dense(3)(wb_transform_1)\n",
    "    \n",
    "    wb_transform_2 = ColorTransformLayer()(wb_input)\n",
    "    wb_transform_2 = layers.Dense(3)(wb_transform_2)\n",
    "    wb_transform_2 = layers.Dense(3)(wb_transform_2)\n",
    "\n",
    "    rgb_layers = layers.Identity()(rgb_input)\n",
    "\n",
    "    # Apply a transformed white balance.\n",
    "    rgb_layers = layers.Multiply()([rgb_layers, wb_transform_1])\n",
    "\n",
    "    # Apply some gamma and color curves to the input.\n",
    "    # Using multiple curves or applying curves after color correction\n",
    "    # does not improve performance. Increasing the dimensions may or\n",
    "    # may not improve performance.\n",
    "    rgb_layers = layers.Dense(3)(rgb_layers)\n",
    "    rgb_layers = layers.Dense(6, activation='elu', name='gamma_1')(rgb_layers)\n",
    "    rgb_layers = layers.Dense(6)(rgb_layers)\n",
    "    rgb_layers = layers.Dense(6, activation='sigmoid', name='s_curve_1')(rgb_layers)\n",
    "    rgb_layers = layers.Dense(3)(rgb_layers)\n",
    "    \n",
    "    # Apply the color transformation to the RGB image.\n",
    "    rgb_layers = ColorTransformLayer()(rgb_layers)\n",
    "   \n",
    "    # Apply a second transformation of the white balance to the output.\n",
    "    # This has a minimal but measurable improvement.\n",
    "    rgb_layers = layers.Multiply()([rgb_layers, wb_transform_2])\n",
    "\n",
    "    model = keras.Model(inputs=merged_input, outputs=rgb_layers)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44105681-d333-45de-a138-86d0655735a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_correction_model = create_model()\n",
    "color_correction_model.compile(optimizer='adam', loss='mse')\n",
    "color_correction_model.build((None, 6))\n",
    "color_correction_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695d66a-1a65-4561-93b6-329e97e181dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_correction_model.fit(wb_datasets_combined.batch(32), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8687c5-ebbc-4ce8-a7c7-698078fc3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_correction_model.save_weights(f'color_correction_model_0_1.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9db81-a8b7-4e03-9c49-66b607085bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is too slow to be practical. It must be running operations on single\n",
    "# pixels instead of the image as a batch.\n",
    "\n",
    "def apply_correction_model(img_arr, model, wb_matrix):\n",
    "    r_orig, c_orig = img_arr.shape[:2]\n",
    "\n",
    "    img_linear_arr = to_color_arr(img_arr)\n",
    "\n",
    "    process_arr = np.array([np.concatenate((r, wb_matrix)) for r in img_linear_arr])\n",
    "    display(process_arr.shape)\n",
    "    \n",
    "    output_arr = model.predict(process_arr)\n",
    "    return output_arr.reshape((r_orig, c_orig, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb28ccf-1311-47ca-85d4-ef210b511150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_img = apply_correction_model(test_exr_img_arr, color_correction_model, pt_cloudy_wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c30120-6fac-49dd-991d-0204f1e022d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92100a96-8748-4b79-8b0a-0a40f5bcae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_correction_model(exr_img_arr, color_correction_model, indoor_wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489aa1b-c1e8-4a13-a432-73697337fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(test_srgb_img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661d5b8-a27a-4e50-b281-dda3548607c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For some reason DarkTable is having problems with this EXR. Gimp will produce\n",
    "# usable results when using the eyedropper tool on an image to adjust color\n",
    "# curves, but this is really what we're trying to avoid doing.\n",
    "\n",
    "pyexr.write(\n",
    "    f\"./corrected_output_img.exr\",\n",
    "    np.asarray(output_img),\n",
    "    precision=pyexr.HALF,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e12812-77d4-41c9-b87f-7d8098f341e5",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This approach above is still not working well. I suspect that the model is not\n",
    "using the white balancing layers, which is why it is producing the muddy\n",
    "results, which seem to be too warm or too cool depending on which lighting\n",
    "scenario the images were taken with.\n",
    "\n",
    "### Alternative approaches\n",
    "\n",
    "* Create a color correction model which we co-train to produce a white balance\n",
    "  transform, using as input either:\n",
    "    - the white / black point\n",
    "    - the white balance type\n",
    "    - color correction weights\n",
    "* Create a bunch of color correction sample images using color temperature controlled panel lights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403f27f-f24f-4971-b4ea-d98d2b0e0132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
